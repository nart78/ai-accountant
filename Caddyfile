accounting.johnnytran.ca {
    # Password protection — only you can access this
    # To generate a new hash on the VPS: docker run --rm caddy:2-alpine caddy hash-password
    # Then set BASIC_AUTH_HASH in your .env file
    basicauth {
        {$BASIC_AUTH_USER} {$BASIC_AUTH_HASH}
    }

    # Tell search engines: do not index, do not follow, do not cache
    header {
        X-Robots-Tag "noindex, nofollow, noarchive, nosnippet"
        X-Content-Type-Options nosniff
        X-Frame-Options DENY
        Referrer-Policy no-referrer
        # Prevent the site from appearing in Google's results even if crawled
        -Server
    }

    # Block search engine bots at the server level
    @bots {
        header User-Agent *Googlebot*
        header User-Agent *Bingbot*
        header User-Agent *Slurp*
        header User-Agent *DuckDuckBot*
        header User-Agent *Baiduspider*
        header User-Agent *YandexBot*
        header User-Agent *facebookexternalhit*
    }
    respond @bots 403

    # Compress responses
    encode gzip

    # Proxy API requests to backend
    handle /api/* {
        reverse_proxy backend:8000
    }

    # Proxy everything else to frontend
    handle {
        reverse_proxy frontend:80
    }
}

weatherbot.johnnytran.ca {
    # Password protection — same credentials as accounting app
    basicauth {
        {$BASIC_AUTH_USER} {$BASIC_AUTH_HASH}
    }

    # Tell search engines: do not index, do not follow, do not cache
    header {
        X-Robots-Tag "noindex, nofollow, noarchive, nosnippet"
        X-Content-Type-Options nosniff
        X-Frame-Options DENY
        Referrer-Policy no-referrer
        -Server
    }

    # Block search engine bots at the server level
    @bots {
        header User-Agent *Googlebot*
        header User-Agent *Bingbot*
        header User-Agent *Slurp*
        header User-Agent *DuckDuckBot*
        header User-Agent *Baiduspider*
        header User-Agent *YandexBot*
        header User-Agent *facebookexternalhit*
    }
    respond @bots 403

    # Serve robots.txt that disallows everything
    handle /robots.txt {
        respond "User-agent: *
Disallow: /" 200
    }

    # Compress responses
    encode gzip

    # Proxy all requests to the weather bot container
    reverse_proxy weatherbot:5001
}
